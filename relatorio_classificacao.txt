- Pré-processamento de dados

A primeira questão foi tratar o dado de uma forma geral, retirando colunas que não são interessantes para a análise e retirando linhas com nulo. Isso reduziu um pouco o tamanho do dataframe, perdendo uma coluna e X linhas.

A segunda questão foi como tratar o texto, tanto da Descrição quanto do Título. Existem diversas técnicas para processar o texto, como remoção de stopwords, lemmatization e stemming. Um detalhe importante que foi observado é que nem todas as descrições e títulos estão em inglês. Palavras em idiomas diferentes, como hindi, foram observadas, porém o NLTK não possui stopwords para grande parte delas. Além disso, há uma discussão entre lemmatization e stemming, normalmente indicando qual das duas utilizar. No entanto, foi observado que as duas podem ser utilizadas em conjunto e que isso pode fazer com que o número de features diminua. Para o trabalho, era almejado que o máximo de técnicas para preparar o dado fossem utilizadas. Isso porque nesse caso é interessante reduzir o número de features, para que palavras que poderiam ser generalizadas como uma só não fossem tratadas como features diferentes. 

- Escolha do modelo de feature extraction

Essa foi uma dúvida grande, porque existem várias alternativas e é difícil mudar de ideia depois de executar todo o restante do código. De início, foi considerado utilizar o Bag of Words. Houve uma tentativa de fazer o Bag of Words do zero, dizendo para cada linha quais palavras do vocabulário estavam presentes no combinado do Título + Descrição. No entanto, isso foi logo descartado porque o custo computacional de montar tudo do zero era alto. Foi pensada então em utilizar o CountVectorizer, versão do BoW disponibilizada pelo SKLearn. Por mais que ela tenha sido melhor em termos de desempenho, durante a pesquisa surgiu a ideia do TF-IDF, que possui a mesma ideia do Bag of Words mas ao invés de colocar 0s ou 1, é feito um cálculo que penaliza palavras muito frequentes ou palavras muito raras. Ele assume que essas palavras não são estatisticamente importantes para encontrar um padrão [1]. 

[1] https://www.linkedin.com/pulse/count-vectorizers-vs-tfidf-natural-language-processing-sheel-saket#:~:text=TF%2DIDF%20is%20better%20than,by%20reducing%20the%20input%20dimensions.

Foram consideradas outras abordagens também, como BERT ou Word2Vec. Mas os dois casos não foram considerados porque não só seria um maior custo computacional como também seria um problema para juntar as informações de Título e Descrição, já que nesse caso a posição das palavras no texto importa. Pro contexto de classificar as categorias, foi entendido que as duas colunas têm textos muito similares e que a sua junção poderia ser encarada como uma possibilidade de detectar quais palavras são menos importantes.

- Separação entre dados de treinamento e de teste

Nessa etapa, uma das ideias era usar K-Fold Cross Validation pois esse método traz uma segurança maior em relação às métricas. No entanto, como se trata de um problema de classificação, foi utilizado o StratifiedKFold, que é uma versão do K-Fold com o intuito de garantir que a porcentagem de cada classe nos dados de treinamento e de teste estejam equilibrados. Não só isso como o shuffle foi deixado com o True para que as amostras por classe sejam embaralhadas antes de serem separadas nos lotes [2].

Um detalhe interessante que ocorreu durante o trabalho foi a detecção de um possível Data Leakage. Isso porque o vectorizer do TF-IDF era feito antes da separação entre dados de treinamento e de teste. Dessa forma, os dados de treinamento compartilhavam do mesmo vocabulário que os de teste. Por isso, a função cross_validate que era utilizada antes para verificar as métricas dos modelos teve que ser resubstituída por uma análoga (cross_val_tfidf) que faz um TF-IDF para os dados de treinamento dentro de um fold e outro para os de teste [3].

[2] https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html
[3] https://stackoverflow.com/questions/46010617/do-i-use-the-same-tfidf-vocabulary-in-k-fold-cross-validation

- Avaliação dos modelos

Para verificar qual dos modelos obteve melhor performance, foram considerados: Regressão Logística, Gradiente Estocástico Descendente, Perceptron, SVC (Support Vector Machine), Árvore de Decisão, Random Forest, Naive Bayes e KNN. O ideal seria fazer ajuste de hiperparâmetros em todos eles, porém nem todos indicavam que os hiperparâmetros iriam ajudar a levar o modelo a ser escolhido e acabou que os primeiros acabaram dando os melhores resultados em termos de performance mesmo. Por isso, apenas alguns tiveram seus hiperparâmetros ajustados. 

No caso da Regressão Logística, é preciso levar em consideração que ela assume que os dados podem ser separados linearmente. Dessa forma, a dimensionalidade deve ser a mais alta possível e ter um alto número de features faz com que os dados possam ser separados em uma dimensão maior. [4]

Como o resultado já havia sido bom sem mexer nos hiperparâmetros, foi feito o tuning deles para que nós conseguissemos obter uma acurácia ainda maior. Isso foi obtido com o solver linlinear, C = 100 e a penalidade para L2. 

Abaixo, temos os resultados. 

- Escolha do modelo e matriz de confusão

De fato, o modelo escolhido seria o de Regressão Logística, que possui tanto acurácia quanto log loss bastante satisfatórios. Pela acurácia dos dados de teste, não existe uma diferença absurda para a acurácia dos dados de treinamento (100%) que sugeririam overfitting. 
Para enxergar melhor o resultado do modelo, foi plotada a Matriz de Confusão (cujas funções mostradas em sala de aula foram utilizadas paara considerar a matriz a partir da Stratified K-Fold Cross Validation). Note que o resultado é bom, pois as diagonais estão com a maior concentração. Isso indica que o modelo faz mais previsão correta do que incorreta além de não parecer ter nenhum problema específico com alguma classe.

[4] https://medium.com/@akshayc123/logistic-regression-87f7fbb4aaf6#:~:text=Logistic%20Regression%20(LR)%20is%20a,well%20on%20linearly%20separable%20classes.
